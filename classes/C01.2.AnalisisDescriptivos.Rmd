---
title: "Unidad 1.2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(moments)
library(modeest)
library(gplots)
library(MASS)
library(nortest)
```

# Unidad I. Variables, distribuciones y pruebas de hipótesis.

- Características numéricas de las variables aleatorias
- Concepto de muestra
- Estimación estadística de los parámetros de una distribución a partir de los
datos de una muestra

## Análisis descriptivo de datos

La estadística descriptiva tiene más que ver con describir una muestra de manera
cualitativa (gráfica) o cuantitativa (numérica) que con inferir propiedades
acerca de la población de la cual proviene esa muestra (estadística inferencial).

La estadística descriptiva está fuertemente ligada al análisis exploratorio de
datos (EDA for Exploratory Data Analysis) y al análisis inicial de datos (IDA
por Initial Data Analysis). El primero focaliza en explorar los datos en busca
de nuevas hipótesis, las cuales pueden terminar en nuevos muestreos y 
experimentos, mientras el segundo se focaliza en asegurar la calidad de los 
datos, chequear las asunciones y realizar las transformaciones necesarias para 
testear la hipótesis que teníamos en mente a la hora de recolectar los datos.

El análisis inicial de datos no es necesario solamente para el testeo de 
hipótesis de la estadística inferencial, sino también como un paso previo para 
el aprendizaje automático (ML por Machine Learning) y forma parte importante de 
las primeras fases de la minería de datos (Data mining).

### Muestra estadística

Se denomina muestra a un subconjunto de datos tomados o seleccionados de una 
población estadística mediante un proceso de muestreo determinado. Cada una de 
las unidades muestrales suele llamarse observación, y es posible medir variables
aleatorias para cada una de ellas.

Las **muestras** pueden ser:  
- **Completas**: Incluye a todos los casos, individuos u objetos de la población
que cumplen con un criterio (de selección) determinado. Generalmente es difícil
o imposible disponer de muestras completas.  
- **Representativas** (*representative or unbiased*): Un conjunto de unidades
muestrales seleccionados de una muestra completa usando un proceso de
selección/muestreo que no depende de las propiedades de estas unidades. Una
manera de obtener muestras no sesgadas es seleccionando una
**muestra aleatoria** (*random/probability sample*). 

#### Técnicas de muestreo

##### Muestreo aleatorio simple

En el muestreo aleatorio simple se selecciona un número k de unidades muestrales
de manera aleatoria, teniendo cada elemento de la población la misma 
probabilidad de ser seleccionado. El muestreo puede ser con o sin reposición
(with or without replacement). Si el muestreo se efectúa con reposición, cada
elemento de la población puede ser seleccionado más de una vez. En el muestreo
aleatorio sin reposición, donde los elementos no son devueltos a la población y
no pueden ser elegidos más de una vez, la probabilidad de sacar un determinado
elemento cambia con la extracción del anterior (no son independientes). Sin
embargo el muestreo aleatorio simple sin reposición satisface
intercambiabilidad, es que cualquier orden de los elementos extraídos es
igualmente probable. Si el tamaño de la población es mucho más grande que el
tamaño de la muestra, el muestreo aleatorio simple sin reposición se aproxima a
un muestreo simple con reposición, dada la baja probabilidad de elegir un mismo
elemento dos veces.

La manera más sencilla de hacer un muestreo aleatorio simple en R con o sin
reposición es usando la función sample R. Toma una lista de valores, el tamaño
de la muestra a tomar (size) y se les debe indicar si queremos que sea con o
sin reemplazo o reposición (replace). Una diferencia entre ambas funciones es
que mientras R por defecto hace el muestreo sin reemplazo.

##### Muestreo sistemático

El muestreo sistemático consiste en ordenar los elementos de una población según
alguna variable de interés, para luego tomar n unidades muestrales
equiespaciadas. El primer elemento debe ser seleccionado al azar, quedando los
otros determinados en relación a este. Una ventaja de este tipo de muestreo es
que permite muestrear una variable de intereses en todo su rango. Pero debe
tenerse cuidado si la variable muestra alguna característica periódica, dado que
puede generarse una muestra sesgada de la población. Además debe tenerse en
cuenta que no se verá la variación entre dos elementos contiguos en la lista,
dado que nunca son seleccionados a la vez.

Por ejemplo, si queremos hacer un muestreo sistemático de las plantas del
conjunto de datos iris según largo de sus pétalos:

```{r}
library(datasets)
data(iris)
head(iris)
```

```{r}
iris <- iris[order(iris$Petal.Length),]
pop_size <- nrow(iris)
samples <- 7 # Número de muestras
interval <- pop_size %/% samples
starting_position <- sample(0:(interval-1), 1)
sample_indexes <- (1:pop_size %% interval) == starting_position
iris_samples <- iris[sample_indexes, ]
iris_samples

```

##### Muestreo estratificado

En el muestreo estratificado, se estratifica la población antes de tomar las
muestras. En este proceso se divide a los miembros de la población en estratos,
grupos o subpoblaciones homogéneas. Estos estratos deben ser mutuamente
excluyentes, dado que un miembro de la población no podrá pertenecer a más de
una subpoblación. Todos los miembros de la población deben pertenecer a un
estrato determinado, no pueden quedar miembros sin clasificar (estratificación
exhaustiva). Una vez que se estratifico la población, se realizar un muestreo
aleatorio simple o sistemático  dentro de cada estrato.

Existen tres posibles estrategias:  
- **Asignación proporcional**: El número de unidades muestrales de cada estrato
  es proporcional al número de individuos del estrato dentro de la población. Es
  decir, se respeta las proporciones de los estratos en la población.  
- **Asignación óptima**: El número de muestras seleccionadas para cada estrato
  es proporcional a la desviación estándar de la variable de interés en cada
  estrato. Ésto requiere un conocimiento previo de la población.  
- **Asignación uniforme**: Se selecciona un igual número de elementos para cada
  estrato. Ésto permite, cuando el tamaño de los estratos varía en la población,
  equiparar el poder de los test utilizados en cada estratos a la hora de
  compararlos. El conjunto de datos *iris* fue generado en un muestreo
  estratificado uniforme (50 flores de cada especie), en el que cada especie
  conforma un estrato.

La media del muestreo es:

$$ \mu_{s} = \frac{1}{N} \sum_{h=1}^{L} N_{h} \mu_{h} $$
donde:
* mu_{s} es la media
* N es el tamaño de toda la población
* N_{h} es el tamaño del estrato h
* \mu_h es la media de la muestra del estrato h.

```{r}
my_sample_size <- 30
selected_indexes <- c()
for (sp in levels(iris$Species)) {
  current_selectes <- sample(rownames(iris[iris$Species==sp,]), my_sample_size)
  selected_indexes <- c(selected_indexes, current_selectes)
  my_sample_size <- my_sample_size + 5
}
iris2 <- iris[selected_indexes, ]
```

```{r}
# Asignación proporcional
strate_size <- sapply(
  levels(iris$Species),
  function(x) nrow(iris2[iris2$Species==x, ]))

strate_sample_size <- strate_size * 0.25

selected_indexes <- c()
for (sp in levels(iris2$Species)) {
  current_selected <- sample(
    rownames(iris2[iris2$Species==sp, ]),
    as.integer(strate_sample_size[sp]))
  selected_indexes <- c(
    selected_indexes,
    current_selected)
}
stratified <- iris2[selected_indexes, ]
head(stratified)
```
```{r}
estratos <- data.frame(
  row.names = levels(stratified$Species),
  Nh=strate_size,
  muh=sapply(
    levels(stratified$Species),
    function(x) mean(stratified[stratified$Species==x, "Petal.Length"])
  )
)
estratos
```

```{r}
mu_g <- sum(estratos$Nh*estratos$muh) / sum(estratos$Nh)
```


### Estadísticos de resumen

Los estadísticos de resumen (*Summary statistics*) describen de manera
cuantitativa la distribución de una muestra. Normalmente se obtiene un conjunto
de estadísticos que describen cada variable aleatoria/dimensión de los datos de
manera independiente (si excluimos medidas de dependencia como las
correlaciones).  

Existen estadísticos de:

- tendencia central (location or central tendency)
- dispersión (spread or dispersion)
- forma (shape):
  - asimetría (Skewness)
  - apuntamiento (Kurtosis)

Estos estadísticos pueden ser robustos o no, llamándose robustos a los
estadísticos menos afectados por valores atípicos (outliers). Estos estadísticos
describen la muestra de una manera más robusta cuando su distribución se aleja
de una distribución normal (por ejemplo, si la distribución es asimétrica).

#### Estadísticos de tendencia central

Existen diversos estadísticos de tendencia central, siendo la *media* el más
popular de ellos a pesar de no ser un estadístico robusto. La medidas de
ubicación robusta más popular es la *mediana*. La *moda* es el único estadístico de
tendencia central para datos nominales, pero es difícil de estimar correctamente
para variables continuos.


```{r}
iris2
```


```{r}
for (c in colnames(iris2)) {
  print(c)
  c_column <- iris2[,c]
  if (is.numeric(c_column)) {
    print(paste0("  - Media   : ", mean(c_column)))
    print(paste0("  - Mediana : ", median(c_column)))
  }
  print(paste0("  - Moda    : ", mlv(c_column, method="mfv")))
}
```

#### Estadísticos de dispersión

De los estadísticos de dispersión, la *desviación estándar* es el más popular
acompañando a la media. No debe confundirse con el error estadístico o estándar,
que en realidad habla de la dispersión de las medias muestrales, y no de la
variable de interés. La desviación estándar no es un estimador robusto.
Alternativas más robustas son el rango entre cuartiles y la **Median Absolute
Deviation** (MAD).

- Desviación standard: $$s=\sqrt{\frac{\sum_{i=1}^N{(x_i-\bar{x})^2}}{N-1}}$$
- Varianza: $$s^2$$
- Rango inter quartil: $$Q_3-Q_1$$
- Desviación media absoluta: $$mediana(|X_i-mediana(X)|)$$

```{r}
for (c in colnames(iris2)) {
  print(c)
  c_column <- iris2[,c]
  if (is.numeric(c_column)) {
    print(paste0("  - Desv. Std.   : ", sd(c_column)))
    print(paste0("  - Varianza : ", var(c_column)))
    print(paste0("  - IQR : ", IQR(c_column)))
    print(paste0("  - MAD : ", mad(c_column)))}
}
```

#### Estadísticos de forma

Las dos medidas de forma de la distribución principales son el *Skewness* o
*asimetría* y la *Kurtosis* que habla de la concentración de datos cerca de la
media y de la probabilidad de observar outliers.

La **kurtosis** de la distribución normal es 3, sin embargo suele usarse la
$kurtosis - 3$ o *excess kurtosis* para comparar contra la distribución normal.
Distribuciones para las cuales es menos probable observar *outliers* que en la
distribución normal, dan valores de kurtosis negativa. Mientras que valores
positivos de kurtosis se observan para distribuciones de probabilidad para los
cuales se obtienen *outliers* o valores extremos con más frecuencia (es decir,
poseen colas más pesadas).

```{r}
# Análogo de dnorm
dtriangular <- function(x, a=1) {
  res <- rep(NA, length(x))
  res[(x>a | x< (-a))] <- 0
  res[0<=x & x<=a] <- (1/a-x[0<=x & x<=a]/a^2)
  res[-a<=x & x<0] <- (x[-a<=x & x<0]+a)/a^2
  return(res)
}
# Análogo de rnorm
rtriangular <- function(n, a=1) {
  r <- runif(n, 0, 1)
  res <- rep(NA, length(n))
  res[r<=1/2] <- a * (sqrt(2*r[r<=1/2])-1)
  res[r>1/2] <- a * (1-sqrt(2*(1-r[r>1/2])))
  return(res)
}

# fit_tri <- function(x) {
#   x <- x[x>=0]
#   h <- hist(x, 100)
#   widths <- h$breaks[2:length(h$breaks)]-h$breaks[1:length(h$breaks)-1]
#   ci <- h$counts/widths
#   ci <- ci / (2*sum(h$counts))
#   ws <- h$breaks[1:length(h$breaks)-1] + widths/2
#   a <- lm(ci ~ ws)
#   print(summary(a))
#   return(sqrt(-1/a$coefficients["ws"]))
# }
# fit_tri(rtriangular(100, 1))

tri_sample <- rtriangular(50000, 1)
hist(tri_sample)
text(1,4000, paste("skewness=",sprintf("%.2e", skewness(tri_sample))), adj=c(1,0.5))
text(1,3500, paste("kurtosis=",sprintf("%.2e", kurtosis(tri_sample))), adj=c(1,0.5))

tri_sample2 <- c(
  rtriangular(25000, 0.1),
  rtriangular(25000, 5))
hist(tri_sample2)
text(4,15000, paste("skewness=",sprintf("%.2e", skewness(tri_sample2))), adj=c(1,0.5))
text(4,14000, paste("kurtosis=",sprintf("%.2e", kurtosis(tri_sample2))), adj=c(1,0.5))

normal_sample <- rnorm(50000)
hist(normal_sample)
text(-4,8000, paste("skewness=",sprintf("%.2e", skewness(normal_sample))), adj=c(0,0.5))
text(-4,7200, paste("kurtosis=",sprintf("%.2e", kurtosis(normal_sample))), adj=c(0,0.5))

poisson_sample <- rpois(50000, 3)
hist(poisson_sample)
text(7,8000, paste("skewness=",sprintf("%.2e", skewness(poisson_sample))), adj=c(1,0.5))
text(7,7200, paste("kurtosis=",sprintf("%.2e", kurtosis(poisson_sample))), adj=c(1,0.5))


uniform_sample <- runif(50000, -2, 2)
hist(uniform_sample, ylim=c(0,7000))
text(0.9,5000, paste("skewness=",sprintf("%.2e", skewness(uniform_sample))), adj=c(1,0.5))
text(0.9,3600, paste("kurtosis=",sprintf("%.2e", kurtosis(uniform_sample))), adj=c(1,0.5))

binomial_sample <- rbinom(50000, 20, 0.1)
hist(binomial_sample, breaks=c(min(binomial_sample):(max(binomial_sample)+1)), include.lowest=FALSE, right=FALSE)
text(10, 14000, paste("skewness=",sprintf("%.2e", skewness(binomial_sample))), adj=c(1,0.5))
text(10, 13000, paste("kurtosis=",sprintf("%.2e", kurtosis(binomial_sample))), adj=c(1,0.5))

```

#### Descripción gráfica

Los *Histogramas* permiten tener una visión de cómo sería la forma de una
distribución de densidad para una variable aleatoria continua. Se construyen
dividiendo la variable en grupos (bins) y contando el número de observaciones
dentro de cada uno (representada por la altura de la barra).

El siguiente paso de complejidad que podríamos dar para tener una mejor
estimación de la función de densidad de probabilidad, es utilizar Averaged
Shifted Histograms (ASH) o el estimador por núcleo (KDE por Kernel Density
Estimator). R posee una función density que puede observarse usado
plot(density(....))

KDE tiene dos parámetros importantes, uno es la función kernel a utilizar que
deber ser una distribución de probabilidad, por defecto se utiliza la
distribución Normal. El otro parámetro es el ancho de banda a utilizar.

```{r}
par(mfrow=c(1,2))
hist(
  x=iris$Sepal.Length,
  xlim=c(0,10),
  ylim=c(0,40),
  col="#AACCFF55",
  main="Histograms of iris")
hist(
  iris$Sepal.Width,
  col="#AAFFCC55",
  add=TRUE)
hist(
  iris$Petal.Length,
  col="#fAdd4455",
  add=TRUE)
hist(
  iris$Petal.Width,
  col="#AF2CCa55",
  add=TRUE)

plot(
  density((iris$Sepal.Length)),
  xlim=c(0,10),
  ylim=c(0,1),
  col="#AACCFF44",
  main="Histograms of iris",
  type="h")
lines(
  density((iris$Sepal.Width)),
  col="#AAFFCC44",
  type="h")
lines(
  density((iris$Petal.Length)),
  col="#fAdd4444",
  type="h")
lines(
  density((iris$Petal.Width)),
  col="#AF2CCa44",
  type="h")

```

Los Diagramas de Dispersión (scatter plots) utilizan las coordenadas cartesianas
para mostrar cómo se distribuyen dos variables en un espacio bi dimensional. Es
posible representar más dimensiones utilizado diferentes formas, tamaños y/o
colores.

```{r}
plot(
  x=iris$Sepal.Length,
  y=iris$Petal.Length,
  col=as.integer(iris$Species)+1,
  pch=19)
legend(
  "topright",
  legend=levels(iris$Species),
  col=unique(as.integer(iris$Species))+1,
  pch=19)

```

```{r}
pairs(
  iris[,1:4],
  pch = 19,
  col=as.integer(iris$Species)+1)
```


Otra manera de observar la distribución conjunta de dos variables continuas es
haciendo uso de los histogramas bivariados o bidimensionales (2D o bivariate
histograms). Los grupos (bins) se establecen para las dos variables, definiendo
rectángulos en un espacio bidimensional. Normalmente se utiliza un código de
color para indicar la cantidad de valores en cada grupo.

```{r}
?hist2d
a <- hist2d(
  x=iris$Sepal.Length,
  y=iris$Sepal.Width,
  nbins = 4,
  xlab="Sepal Length",
  ylab="Sepal Width",
  main="Histogram 2D")
```

Esta representación se asemeja a la de un Mapa de calor (heatmap), pero estos
últimos generalmente poseen variable discretas o categóricas en sus ejes (en
lugar de la discretización de una variable continua, aunque ésto también es
posible). Los heat maps son útiles para mostrar los valores de una matriz, y
representan a una variable continua utilizando un gradiente de colores.

```{r}
mean_by_group <- aggregate(
  iris[,1:4],
  list(iris$Species),
  mean)

mean_matrix <- as.matrix(
  mean_by_group[,2:5])

colnames(mean_matrix) <-  c("SL", "SW", "PL", "PW")
rownames(mean_matrix) <- rownames(mean_matrix) <- mean_by_group$Group.1

heatmap(
  mean_matrix,
  Rowv=NA,
  Colv=NA,
  margins = c(8,2)
  )

```

Los Diagramas de Cajas (boxplots) son otra forma de visualizar datos acerca de
una distribución. Son interesantes porque permiten observar estadísticos
robustos de tendencia central (mediana) y de dispersión (rango entre cuartiles).

```{r}
new_data <- data.frame(
  measure=c(
    rep(
      colnames(iris)[1:4],
      each=nrow(iris))
  ),
  value=do.call(
    what="c",
    args=lapply(
      colnames(iris[1:4]),
      function(x) {return(iris[,x])}
    )
  )
)

boxplot(
  value ~ measure,
  new_data
)

```

Mientras los density plots permiten observar una PDF inferida a partir de los
datos, la función ecdf, permite observar la
función de densidad de probabilidad acumulada empírica de una muestra (Empirical
Cumulative Distribution Function ECDF), una aproximación a la CDF.

```{r}
?ecdf
f <- ecdf(iris$Sepal.Length)
xs <- seq(4,9,0.2)
xs

ys <- f(xs)
ys
plot(
  x=xs,
  y=ys,
  type="l"
  )
```

### Ajuste de distribuciones estadísticas

Es posible, para los datos de una muestra, ajustar una distribución de densidad
de probabilidad (distribution fitting). Muchos de los estadísticos determinados
durante la exploración de datos pueden son los estimadores de máxima
verosimilitud o maximum-likelihood estimator (MLE) de algunas distribuciones.
Por ejemplo, la media es el estimador de máxima verosimilitud de la media
poblacional $\mu$ de una distribución normal. Tanto los estadísticos
cuantitativos, como el análisis gráfico realizado en la etapa de exploración
(histogramas, density plots, boxplots y ecdf) pueden ayudar a decidir cuál
familia de funciones de densidad de probabilidad debemos ajustar.
  
```{r}

par(mfrow=c(1,2))

normal_sample <- rnorm(
  100, 2, 3)

fitted <- fitdistr(
  normal_sample,
  densfun = "normal")
summary(fitted)

fitted$n
hist(
  normal_sample,
  10,
  freq = FALSE,
  ylim=c(0,0.25))

estimated_normal <- function(x){
  dnorm(
    x,
    fitted$estimate["mean"],
    fitted$estimate["sd"]
  )
}
curve(
  estimated_normal, 
  from=-6,
  to=10,
  add=TRUE,
  col="red",
  lwd=3)

xs <- seq(-6,10,0.2)
ys <- ecdf(x = normal_sample)(xs)
plot(
  x=xs,
  y=ys,
  type="l"
  )

estimated_normal_cdf <- function(x){
  pnorm(
    x,
    fitted$estimate["mean"],
    fitted$estimate["sd"]
  )
}

curve(
  estimated_normal_cdf, 
  from=-6,
  to=10,
  add=TRUE,
  col="red")


```

```{r}
normal_sample <- rnorm(
  1000, 10, 3)

fitted <- fitdistr(
  normal_sample,
  densfun = "lognormal")
fitted$loglik

fitted <- fitdistr(
  normal_sample,
  densfun = "normal")
fitted$loglik

```


### Tests de normalidad

Como parte del análisis inicial de datos, muchas veces debemos comprobar si los
valores de una variable en nuestra muestra se distribuyen de manera normal.
Existen diversas manera de testear normalidad, las más populares son:

#### Gráfico de probabilidad normal

Los gráficos QQ (qq plots por quantile) permiten comparar los cuantiles de dos
distribuciones. Cuando en uno de los ejes se colocan los cuantiles de una
distribución normal canónica, hablamos de un gráfico de probabilidad normalidad.
Si la distribución de los datos es aproximadamente normal, el gráfico de
normalidad mostrará los puntos equivalente a los datos distribuidos sobre una
línea.

```{r}
normal_sample <- rnorm(500, 1, 2)
qqnorm(normal_sample)
qqplot(
  normal_sample,
  rt(500, 3))
```

#### Test de hipótesis

Existen diversos test de hipótesis para testear normalidad, estos tienen como
hipótesis nula que los datos provienen de una cierta distribución (en particular
de una distribución normal). Por lo tanto, p values altos son indicativos de que
no hay evidencia suficiente para rechazar que los datos provienen de una
distribución normal. Las dos pruebas de normalidad recomendadas son la de
Anderson-Darling y la prueba de Shapiro-Wilk, ambas poseen más poder que
Kolmogorov-Smirnov para esa tarea.


```{r}
normal_sample <- rnorm(50)
ad.test(normal_sample)
shapiro.test (normal_sample)
```

### Teorización Post Hoc

Se denomina teorización *post hoc* a la generación de hipótesis sugeridas por el
conjunto de datos observado, sin testear esta hipótesis en nuevos datos. Hacerlo
puede resultar en aceptar hipótesis incorrectas, que sólo son válidas en el
presente conjunto de datos, dado que es más probable aceptar una hipótesis
testeada en el conjunto de datos en el cual se genero.

Es necesario testear estas nuevas hipotesis en una nueva muestra de la
población. Sin embargo, en muchas casos eso puede ser imposible, por ejemplo al
analizar un fenómeno natural finito. Este problema de disponer de un conjunto
limitado de datos, haciendo difícil o imposible la recolección de nuevos datos
para la fase de confirmación fue denominado por John Tukey como *uncomfortable
science*.

Un caso no tan obvio de *uncomfortable science* o de teorización *post hoc*
puede surgir bioinformática cuando se toman todos los elementos disponibles de
una base de datos para hacer los análisis exploratorios sin dejar datos
suficientes para testear las hipótesis que surgen de ese análisis. Otro proceso
que puede generar este tipo de problemas es el conocido como data fishing, el
cual consiste en ir testeando hipótesis sobre un conjunto de datos hasta
encontrar una un valor estadísticamente significativo en algún test de hipótesis.

El término *data fishing* posee una connotación negativa, dado que representa el
comportamiento poco ético de buscar, seleccionar e informar sólo los resultados
positivos sin el debido control, muchas veces como si las hipótesis hubiera sido
anterior al análisis (Leung 2011).

Buscar patrones en los datos, muchas veces aplicando test estadísticos, no es
incorrecto. Lo incorrecto es utilizar el mismo conjunto de datos (in-sample data)
para soportar esa hipótesis. Las alternativas posibles para no caer en este
problema son:

- Recolectar nuevos datos (out-of-sample data), realizar un nuevo experimento,
  para testear la nueva hipótesis.
- Si no es posible recolectar nuevos datos, es posible separar el conjunto de
  datos de manera aleatoria en dos grupos. Uno puede ser usado para plantear
  nuevas hipótesis, que luego serán testeadas en el otro conjunto.
- Dado que la teorización *post hoc* y los casos de uncomfortable science pueden
  terminar en sobreajuste (overfitting), los métodos de validación cruzada
  (cross validation) pueden resultar útiles para evitarlo.
- Aplicar correcciones por testeo múltiple, considerando todas las hipótesis
  testeadas (por ejemplo, todas las hipótesis testeadas durante el proceso de
  data fishing).




