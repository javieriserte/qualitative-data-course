{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade matplotlib con Colab\n",
    "# !pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensiones\n",
    "\n",
    "Son métodos numéricos y/o visuales\n",
    "- Representar la variabilidad presente en un conjunto de datos\n",
    "- Con menos variables explicatorias\n",
    "  - No es la simple eliminación de las variables explicatorias existentes\n",
    "  - Es la construcción de un número menor de variables como combinación de las\n",
    "    demás.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de componentes principales\n",
    "\n",
    "- Transformación lineal de los datos en nuevos sistemas de coordenadas.\n",
    "  - El objetivo es que haya algunas de esas dimensiones\n",
    "    - acumulen la mayor parte de información\n",
    "  - Las dimensiones con menos información se eliminan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xa = st.norm.rvs(4, 2.5, size=100)\n",
    "ya = st.norm.rvs(size=100) * 4.5 + 0.9 * xa - 10\n",
    "xb = st.norm.rvs(-4, 2.5, size=100)\n",
    "yb = st.norm.rvs(size=100) * 4.5 + 0.9 * xb + 10\n",
    "\n",
    "np.concatenate((xa, xb))\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    data = {\n",
    "        \"X\" : np.concatenate((xa, xb)),\n",
    "        \"Y\" : np.concatenate((ya, yb))\n",
    "    }\n",
    ")\n",
    "data.head()\n",
    "plt.scatter(data.X, data.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay formación de grupos de datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay grupos en las proyecciones sobre los ejes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x = data.X,\n",
    "    y = [1] * len(data.X)\n",
    ")\n",
    "plt.scatter(\n",
    "    x = data.Y,\n",
    "    y = [2] * len(data.Y)\n",
    ")\n",
    "\n",
    "plt.ylim(0,3)\n",
    "plt.yticks([1, 2], labels = [\"X\", \"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si ratamos los puntos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=7,\n",
    "    figsize = (6, 25)\n",
    ")\n",
    "\n",
    "for i, angle in enumerate([0, 15, 30, 45, 60, 75, 90]):\n",
    "    angle = angle * math.pi / 180\n",
    "    rmat = np.array(\n",
    "        [\n",
    "            [math.cos(angle), -math.sin(angle)],\n",
    "            [math.sin(angle), math.cos(angle)]\n",
    "        ]\n",
    "    )\n",
    "    rotated = (rmat @ data.T).T\n",
    "    axes[i].scatter(\n",
    "        rotated.iloc[:, 0],\n",
    "        rotated.iloc[:, 1]\n",
    "    )\n",
    "    axes[i].set_ylim(-20, 20)\n",
    "    axes[i].set_xlim(-20, 20)\n",
    "\n",
    "    axes[i].scatter(\n",
    "        x = rotated.iloc[:, 0],\n",
    "        y = [-19.5] * len(rotated),\n",
    "        color = \"#ff000044\",\n",
    "    )\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "scale = preprocessing.StandardScaler()\n",
    "scale = scale.fit(data)\n",
    "data_scaled = scale.transform(data)\n",
    "fitted = pca.fit(data_scaled)\n",
    "\n",
    "transformed = fitted.transform(data_scaled)\n",
    "\n",
    "plt.scatter(\n",
    "    x = transformed[:, 0],\n",
    "    y = transformed[:, 1]\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x = transformed[:, 0],\n",
    "    y = [-2] * len(transformed)\n",
    ")\n",
    "plt.xlabel(f\"Component 1: [{fitted.explained_variance_ratio_[0]:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otro ejemplo con tres dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "df = iris[\"frame\"]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "scale = StandardScaler()\n",
    "scale = scale.fit(df.iloc[:, :4])\n",
    "iris_scaled = scale.transform(df.iloc[:, :4])\n",
    "\n",
    "fitted = pca.fit(iris_scaled)\n",
    "transformed = fitted.transform(iris_scaled)\n",
    "scatter = plt.scatter(\n",
    "    x = transformed[:, 0],\n",
    "    y = transformed[:, 1],\n",
    "    c = df[\"target\"],\n",
    ")\n",
    "legend_elements = scatter.legend_elements()\n",
    "plt.legend(\n",
    "    legend_elements[0],\n",
    "    iris[\"target_names\"],\n",
    ")\n",
    "plt.xlabel(f\"Component 1: {fitted.explained_variance_ratio_[0]:0.3f}\")\n",
    "plt.ylabel(f\"Component 2: {fitted.explained_variance_ratio_[1]:0.3f}\")\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de correspondencia multiple\n",
    "\n",
    "- Es una alternativa a PCA, cuando tenemos variables categóricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero construyamos un set de datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "data = [\n",
    "    (\"organism\", [\"homo sapiens\", \"sus scrofa\", \"calomys laucha\", \"xenopus laevis\"]),\n",
    "    (\"disorder\", [\"fully disordered\", \"highly disordered\", \"low disorder\", \"structured\"]),\n",
    "    (\"localization\" , [\"cytoplasm\", \"nucleus\", \"p-granule\", \"mitochondria\", \"reticulus\"]),\n",
    "    (\"llps\" , [\"driver\", \"client\", \"regulator\", \"not involved\"]),\n",
    "    (\"haslcregion\", [\"yes\", \"no\"]),\n",
    "    (\"hasrnabindingdomain\", [\"yes\", \"no\"])\n",
    "]\n",
    "\n",
    "g1 = [\n",
    "    [0.25] * 4,\n",
    "    [0.03, 0.09, 0.40, 0.48],\n",
    "    [0.25, 0.15, 0.01, 0.20, 0.19],\n",
    "    [0.01, 0.09, 0.10, 0.80],\n",
    "    [0.30, 0.70],\n",
    "    [0.25, 0.75]\n",
    "]\n",
    "\n",
    "g2 = [\n",
    "    [0.25] * 4,\n",
    "    [0.60, 0.20, 0.20, 0.00],\n",
    "    [0.05, 0.25, 0.45, 0.15, 0.10],\n",
    "    [0.50, 0.25, 0.18, 0.02],\n",
    "    [0.80, 0.20],\n",
    "    [0.75, 0.25]\n",
    "]\n",
    "\n",
    "g3 = [\n",
    "    [0.25] * 4,\n",
    "    [0.01, 0.10, 0.88, 0.01],\n",
    "    [0.85, 0.04, 0.04, 0.04, 0.03],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.50, 0.50],\n",
    "    [0.50, 0.50]\n",
    "]\n",
    "\n",
    "groups = [g1, g2, g3]\n",
    "gs = [0.35, 0.35, 0.30]\n",
    "\n",
    "n = 1000\n",
    "\n",
    "gr = st.multinomial.rvs(1, gs, n)\n",
    "gr = np.apply_along_axis(np.argmax, 1, gr)\n",
    "\n",
    "def create_sample_from_group(group):\n",
    "    values = [group]\n",
    "    for (var, labels), frq in zip(data, groups[group]):\n",
    "        lbl_index = np.apply_along_axis(np.argmax, 1, st.multinomial.rvs(1,frq,1))\n",
    "        values.append(labels[lbl_index[0]])\n",
    "    return values\n",
    "\n",
    "sim_data = map(create_sample_from_group, gr)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data = sim_data,\n",
    "    columns = [\"group\"] + [x[0] for x in data]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "mca = prince.MCA(n_components=6)\n",
    "fitted = mca.fit(df.iloc[:, 1:])\n",
    "transformed = mca.transform(df.iloc[:, 1:])\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    transformed.iloc[:, 0],\n",
    "    transformed.iloc[:, 1],\n",
    "    c = df[\"group\"],\n",
    "    s = 10\n",
    ")\n",
    "legend_elements = scatter.legend_elements()\n",
    "plt.legend(legend_elements[0], [\"G1\", \"G2\", \"G3\"])\n",
    "plt.xlabel(F\"Dimension 1 (Inertia: {100*fitted.explained_inertia_[0]:0.3f})%\")\n",
    "plt.ylabel(F\"Dimension 2 (Inertia: {100*fitted.explained_inertia_[1]:0.3f})%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos asignar coordenadas a las categorias, no solo a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_coordinates = fitted.column_coordinates(\n",
    "    df.iloc[:, 1:]\n",
    ")\n",
    "column_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_coordinates.index.map(lambda x: x.startswith(\"disorder_\"))\n",
    "disorder_col = column_coordinates.iloc[\n",
    "    column_coordinates.index.map(lambda x: x.startswith(\"disorder_\")),\n",
    "    [0, 1]\n",
    "]\n",
    "disorder_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "colors = [\"green\", \"blue\", \"orange\", \"violet\"]\n",
    "cmap, norm = matplotlib.colors.from_levels_and_colors(\n",
    "    np.arange(len(colors)),\n",
    "    colors,\n",
    "    extend = \"max\"\n",
    ")\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    disorder_col.iloc[:, 0],\n",
    "    disorder_col.iloc[:, 1],\n",
    "    c = np.arange(len(disorder_col)),\n",
    "    marker = \"X\",\n",
    "    cmap = cmap,\n",
    "    norm = norm,\n",
    "    s = 100\n",
    ")\n",
    "legend_elements = scatter.legend_elements()\n",
    "legend_labels = disorder_col.index.map(lambda x: x.split(\"_\")[1])\n",
    "plt.legend(\n",
    "    legend_elements[0],\n",
    "    legend_labels\n",
    ")\n",
    "plt.scatter(\n",
    "    transformed.iloc[:, 0],\n",
    "    transformed.iloc[:, 1],\n",
    "    s = 5,\n",
    "    c = df[\"group\"],\n",
    "    alpha = 0.45\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2642d409aca2f3e50c4cc7e12f906abb96a179c2b76bcb58c2b6a9943c6a4da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
